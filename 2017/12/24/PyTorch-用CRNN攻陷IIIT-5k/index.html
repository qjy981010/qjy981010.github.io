<!DOCTYPE html>
<html lang="zh_CN">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="boom boom boom">
    

    <!--Author-->
    
        <meta name="author" content="Jiyang Qi">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="PyTorch文字识别 用CRNN攻陷IIIT-5k"/>
    

    <!--Open Graph Description-->
    
        <meta property="og:description" content="boom boom boom" />
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Tensor Lover"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

        <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>PyTorch文字识别 用CRNN攻陷IIIT-5k - Tensor Lover</title>

    <!-- Bootstrap Core CSS -->
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"/>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-111519148-1', 'auto');
        ga('send', 'pageview');

    </script>



    <!-- favicon -->
    
    <link rel="icon" href="/img/ai.ico">
    <link rel="shortcut icon" href="/img/ai.ico">
    
	
</head>


<body>

    <!-- Menu -->
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">TensorLover's Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                
                    <li>
                        <a href="/">
                            
                                Home
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/archives">
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/tags">
                            
                                Tags
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/categories">
                            
                                Categories
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="https://github.com/qjy981010">
                            
                                <i class="fa fa-github fa-stack-2x"></i>
                            
                        </a>
                    </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>


    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header" style="background-image: url('http://www.codeblocq.com/assets/projects/hexo-theme-clean-blog/img/home-bg.jpg')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>PyTorch文字识别 用CRNN攻陷IIIT-5k</h1>
                    
                    <span class="meta">
                        <!-- Date and Author -->
                        
                            Posted by Jiyang Qi on
                        
                        
                            2017-12-24
                        
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Tags and categories -->
           
                <div class="col-lg-4 col-lg-offset-2 col-md-5 col-md-offset-1 post-tags">
                    
                        


<a href="/tags/pytorch/">#pytorch</a> <a href="/tags/crnn/">#crnn</a> <a href="/tags/IIIT-5k/">#IIIT-5k</a> <a href="/tags/OCR/">#OCR</a> <a href="/tags/文字识别/">#文字识别</a>


                    
                </div>
                <div class="col-lg-4 col-md-5 post-categories">
                    
                </div>
            

            <!-- Gallery -->
            

            <!-- Post Main Content -->
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <p>CRNN是2015年提出的一种，端对端的，场景文字识别方法，它采用CNN与RNN的结合来进行学习。它相对于其他算法主要有以下两个特点：</p>
<ol>
<li>端对端训练，直接输入图片给出结果，而不是把多个训练好的模型进行组合来识别</li>
<li>不需要对图片中的文字进行分割就可以进行识别，可以适应任意长度的序列</li>
</ol>
<p>原论文在<a href="https://arxiv.org/abs/1507.05717" target="_blank" rel="noopener">这里</a><br>PS:<strong>是CRNN，不是RCNN</strong>，RCNN是一种物体检测算法，别混了。。</p>
<p>本文将重点介绍CRNN原理，以及如何用pytorch实现CRNN，并在IIIT-5k数据集上进行尝试</p>
<h1 id="CRNN解析与构建"><a href="#CRNN解析与构建" class="headerlink" title="CRNN解析与构建"></a>CRNN解析与构建</h1><p>首先让我们看看CRNN的网络总体架构，如下图：<br><img src="/images/network_architecture.png" alt=""><br>自底向上步骤为：</p>
<ol>
<li>通过卷积层提取图像特征</li>
<li>循环层，预测下一帧的字母</li>
<li>转录，将预测序列转化为字母，得到单词</li>
</ol>
<p>对于输入的图片，图片首先通过CNN网络，得到特征图。之后，如何将这个特征图送入RNN呢？CRNN将特征图的每一列像素作为一个特征向量，所有列组成一个特征序列，这一序列将作为RNN的输入，即RNN第i个特征向量为特征图第i列，如下图所示。<br><img src="/images/receptive_field.png" alt=""><br>图中 Feature Sequence 就是特征序列， Receptive Field 就代表原输入图像中的一列（感受野），他们一一对应，且相对位置不变。即原图像上从左到右的每一列，映射到特征序列上，依然保持原来从左到右的顺序。因此特征序列就可以认为是原图像的一种表示。</p>
<p>也正因为这样一种机制，图片的宽度不一定相同，但高度必须相同。为了方便，我们可以调整输入的图片的高度为32，来保证卷积后得到的特征图的每一列都只有一个像素。</p>
<p>CRNN具体的网络结构如下：</p>
<p>注意：为了与论文保持一致，本文的宽高结构均用<strong>宽 × 高</strong>来表示，三维张量格式为<strong>宽 × 高 × 通道数</strong><br><em>其中k表示卷积核大小(kernel_size)，s表示步长(stride)，p表示填充(padding_size)</em></p>
<table>
<thead>
<tr>
<th style="text-align:center">Type</th>
<th style="text-align:center">Configurations</th>
<th style="text-align:center">Output Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Input</td>
<td style="text-align:center">W × 32 gray-scale image</td>
<td style="text-align:center">W × 32 × 1</td>
</tr>
<tr>
<td style="text-align:center">Convolution</td>
<td style="text-align:center">#maps:64, k:3 × 3, s:1, p:1</td>
<td style="text-align:center">W × 32 × 64</td>
</tr>
<tr>
<td style="text-align:center">MaxPooling</td>
<td style="text-align:center">Window:2 × 2, s:2</td>
<td style="text-align:center">W/2 × 16 × 64</td>
</tr>
<tr>
<td style="text-align:center">Convolution</td>
<td style="text-align:center">#maps:128, k:3 × 3, s:1, p:1</td>
<td style="text-align:center">W/2 × 16 × 128</td>
</tr>
<tr>
<td style="text-align:center">MaxPooling</td>
<td style="text-align:center">Window:2 × 2, s:2</td>
<td style="text-align:center">W/4 × 8 × 128</td>
</tr>
<tr>
<td style="text-align:center">Convolution</td>
<td style="text-align:center">#maps:256, k:3 × 3, s:1, p:1</td>
<td style="text-align:center">W/4 × 8 × 256</td>
</tr>
<tr>
<td style="text-align:center">Convolution</td>
<td style="text-align:center">#maps:256, k:3 × 3, s:1, p:1</td>
<td style="text-align:center">W/4 × 8 × 256</td>
</tr>
<tr>
<td style="text-align:center">MaxPooling</td>
<td style="text-align:center">Window:1 × 2, s:2</td>
<td style="text-align:center">W/4 × 4 × 256</td>
</tr>
<tr>
<td style="text-align:center">Convolution</td>
<td style="text-align:center">#maps:512, k:3 × 3, s:1, p:1</td>
<td style="text-align:center">W/4 × 4 × 512</td>
</tr>
<tr>
<td style="text-align:center">BatchNormalization</td>
<td style="text-align:center">-</td>
<td style="text-align:center">W/4 × 4 × 512</td>
</tr>
<tr>
<td style="text-align:center">Convolution</td>
<td style="text-align:center">#maps:512, k:3 × 3, s:1, p:1</td>
<td style="text-align:center">W/4 × 4 × 512</td>
</tr>
<tr>
<td style="text-align:center">BatchNormalization</td>
<td style="text-align:center">-</td>
<td style="text-align:center">W/4 × 4 × 512</td>
</tr>
<tr>
<td style="text-align:center">MaxPooling</td>
<td style="text-align:center">Window:1 × 2, s:2</td>
<td style="text-align:center">W/4 × 2 × 512</td>
</tr>
<tr>
<td style="text-align:center">Convolution</td>
<td style="text-align:center">#maps:512, k:2 × 2, s:1, p:0</td>
<td style="text-align:center">W/4-1 × 1 × 512</td>
</tr>
<tr>
<td style="text-align:center">Map-to-Sequence</td>
<td style="text-align:center">-</td>
<td style="text-align:center">W/4-1 × 512</td>
</tr>
<tr>
<td style="text-align:center">Bidirectional-LSTM</td>
<td style="text-align:center">#hidden units:256</td>
<td style="text-align:center">W/4-1 × 256</td>
</tr>
<tr>
<td style="text-align:center">Bidirectional-LSTM</td>
<td style="text-align:center">#hidden units:256</td>
<td style="text-align:center">W/4-1 × label_num</td>
</tr>
<tr>
<td style="text-align:center">Transcription</td>
<td style="text-align:center">-</td>
<td style="text-align:center">str</td>
</tr>
</tbody>
</table>
<p>下面我们把每个步骤分开来看</p>
<h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p>从上表的配置可以看出，卷积层很像VGG-11。不同的地方主要有两个：</p>
<ol>
<li>增加了批归一化层</li>
<li>池化层的大小从正方形变成了长方形</li>
</ol>
<p>加入批归一化层可以加快训练。而用高为2宽为1的长方形更容易获取窄长英文字母的特征，这样更容易区分像i和l这样的字母。</p>
<h3 id="循环"><a href="#循环" class="headerlink" title="循环"></a>循环</h3><p>循环层采用深度双向LSTM模型，想多了解LSTM的朋友可以看一下<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">这个博客</a></p>
<p>了解了以上两个部分以后，我们就可以开始构建我们的CRNN网络了。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CRNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    CRNN模型</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        in_channels (int): 输入的通道数，如果是灰度图则为1，如果没有灰度化则为3</span></span><br><span class="line"><span class="string">        out_channels (int): 输出的通道数（类别数），即样本里共有多少种字符</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels)</span>:</span></span><br><span class="line">        super(CRNN, self).__init__()</span><br><span class="line">        self.in_channels = in_channels</span><br><span class="line">        hidden_size = <span class="number">256</span></span><br><span class="line">        <span class="comment"># CNN 结构与参数</span></span><br><span class="line">        self.cnn_struct = ((<span class="number">64</span>, ), (<span class="number">128</span>, ), (<span class="number">256</span>, <span class="number">256</span>), (<span class="number">512</span>, <span class="number">512</span>), (<span class="number">512</span>, ))</span><br><span class="line">        self.cnn_paras = ((<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), (<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                          (<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), (<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line">        <span class="comment"># 池化层结构</span></span><br><span class="line">        self.pool_struct = ((<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">2</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">1</span>), <span class="keyword">None</span>)</span><br><span class="line">        <span class="comment"># 是否加入批归一化层</span></span><br><span class="line">        self.batchnorm = (<span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">False</span>, <span class="keyword">True</span>, <span class="keyword">False</span>)</span><br><span class="line">        self.cnn = self._get_cnn_layers()</span><br><span class="line">        <span class="comment"># RNN 两层双向LSTM。pytorch中LSTM的输出通道数为hidden_size * num_directions,这里因为是双向的，所以num_directions为2</span></span><br><span class="line">        self.rnn1 = nn.LSTM(self.cnn_struct[<span class="number">-1</span>][<span class="number">-1</span>], hidden_size, bidirectional=<span class="keyword">True</span>)</span><br><span class="line">        self.rnn2 = nn.LSTM(hidden_size*<span class="number">2</span>, hidden_size, bidirectional=<span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># 最后一层全连接</span></span><br><span class="line">        self.fc = nn.Linear(hidden_size*<span class="number">2</span>, out_channels)</span><br><span class="line">        <span class="comment"># 初始化参数，不是很重要</span></span><br><span class="line">        self._initialize_weights()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span>           <span class="comment"># input: height=32, width&gt;=100</span></span><br><span class="line">        x = self.cnn(x)             <span class="comment"># batch, channel=512, height=1, width&gt;=24</span></span><br><span class="line">        x = x.squeeze(<span class="number">2</span>)            <span class="comment"># batch, channel=512, width&gt;=24</span></span><br><span class="line">        x = x.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)      <span class="comment"># width&gt;=24, batch, channel=512</span></span><br><span class="line">        x = self.rnn1(x)[<span class="number">0</span>]         <span class="comment"># length=width&gt;=24, batch, channel=256*2</span></span><br><span class="line">        x = self.rnn2(x)[<span class="number">0</span>]         <span class="comment"># length=width&gt;=24, batch, channel=256*2</span></span><br><span class="line">        l, b, h = x.size()</span><br><span class="line">        x = x.view(l*b, h)          <span class="comment"># length*batch, hidden_size*2</span></span><br><span class="line">        x = self.fc(x)              <span class="comment"># length*batch, output_size</span></span><br><span class="line">        x = x.view(l, b, <span class="number">-1</span>)        <span class="comment"># length&gt;=24, batch, output_size</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建CNN层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_cnn_layers</span><span class="params">(self)</span>:</span></span><br><span class="line">        cnn_layers = []</span><br><span class="line">        in_channels = self.in_channels</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.cnn_struct)):</span><br><span class="line">            <span class="keyword">for</span> out_channels <span class="keyword">in</span> self.cnn_struct[i]:</span><br><span class="line">                cnn_layers.append(</span><br><span class="line">                    nn.Conv2d(in_channels, out_channels, *(self.cnn_paras[i])))</span><br><span class="line">                <span class="keyword">if</span> self.batchnorm[i]:</span><br><span class="line">                    cnn_layers.append(nn.BatchNorm2d(out_channels))</span><br><span class="line">                cnn_layers.append(nn.ReLU(inplace=<span class="keyword">True</span>))</span><br><span class="line">                in_channels = out_channels</span><br><span class="line">            <span class="keyword">if</span> (self.pool_struct[i]):</span><br><span class="line">                cnn_layers.append(nn.MaxPool2d(self.pool_struct[i]))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*cnn_layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_initialize_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">                n = m.kernel_size[<span class="number">0</span>] * m.kernel_size[<span class="number">1</span>] * m.out_channels</span><br><span class="line">                m.weight.data.normal_(<span class="number">0</span>, np.sqrt(<span class="number">2.</span> / n))</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    m.bias.data.zero_()</span><br><span class="line">            <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">                m.weight.data.fill_(<span class="number">1</span>)</span><br><span class="line">                m.bias.data.zero_()</span><br></pre></td></tr></table></figure></p>
<p>上面网络结构的定义可能有点不是很一目了然，但是感觉这样的代码比较容易维护，而且可复用。</p>
<h3 id="转录"><a href="#转录" class="headerlink" title="转录"></a>转录</h3><p>网络构建完了，接下来是我们最后的转录过程。在实际模型的训练中，我们需要计算损失，然后根据损失来更新参数。这里我们要用到的损失函数是CTC Loss，这一损失函数比较适合用于我们这种序列数据。</p>
<p>在我写这篇博客时pytorch官方还没有提供计算CTC Loss的API，但是pytorch开发人员已经基于百度的warp-ctc，实现了其pytorch版本，这就是我们本次要用的库。（当然你也可以选择其他库，不过缺点就是其他库速度会慢）</p>
<p>我们需要手动编译安装这个库，安装的过程可能会非常坑，大家要有耐心。下面的步骤如果出现奇怪的问题，可以看一下<a href="https://github.com/SeanNaren/warp-ctc/tree/pytorch_bindings/pytorch_binding" target="_blank" rel="noopener">这个库的pytorch-binding的README</a>，或者<a href="https://github.com/baidu-research/warp-ctc/issues" target="_blank" rel="noopener">原百度库的issue</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先把库clone下来</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/SeanNaren/warp-ctc</span><br></pre></td></tr></table></figure></p>
<p>要注意，这个库比较坑的第一点是，编译时必须保证gcc版本在6.0以下，如何降级请大家参考自己Linux发行版的教程。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> warp-ctc</span><br><span class="line">mkdir build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake .. <span class="comment"># 有CUDA的朋友这一步应该能检测到CUDA</span></span><br><span class="line">make</span><br></pre></td></tr></table></figure></p>
<p>编译完以后，你需要把你的gcc版本还原回去，不然后面会出问题。然后，你要把<code>CUDA_HOME</code>这个环境变量设为你CUDA的安装位置，比如大部分人的安装位置应该是在<code>/usr/local/cuda</code>，archlinux是在<code>/opt/cuda</code>，所以把下面一句加到<code>~/.bashrc</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_HOME=<span class="string">"path/to/your/cuda"</span></span><br></pre></td></tr></table></figure></p>
<p>然后开始安装pytorch依赖；<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> pytorch_binding</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure></p>
<p>然后为了保证warpctc_pytorch能被找到，将下面一行加到<code>~/.bashrc</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="string">'/path/to/your/python3.6/site-packages/warpctc_pytorch'</span></span><br></pre></td></tr></table></figure></p>
<p>然后可以试着import一下看能不能用：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> warpctc_pytorch <span class="keyword">import</span> CTCLoss</span><br></pre></td></tr></table></figure></p>
<p>不OK的话可以看上面给的两个链接，或者根据报错适当的改一改他的源码。OK的话就非常棒了。</p>
<p>损失函数有了，但是我们数据集中的标签是字符串，这些字符串是无法直接计算损失的，想将他们转化为网络能用的真正的label，我们要将其按一定的格式编码为数字来进行训练。最后从网络中得到结果后，我们又要将这个结果解码，才得到我们想要的字符串。这个解码的过程就是最后的Transcription。</p>
<p>首先我们要知道有哪些字符需要我们编码，在IIIT-5K中，我们的label中的字符有A-Z，0-9，还有别忘了空字符。一共37个。</p>
<p>我们用一个类来实现编码解码。要注意的是，因为我们所用的库的实现中，默认将空字符编码为0，所以我们要为其余字符设置从1开始的编码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelTransformer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    字符编码解码器</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        letters (str): 所有的字符组成的字符串</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, letters)</span>:</span></span><br><span class="line">        self.encode_map = &#123;letter: idx+<span class="number">1</span> <span class="keyword">for</span> idx, letter <span class="keyword">in</span> enumerate(letters)&#125;</span><br><span class="line">        self.decode_map = <span class="string">' '</span> + letters</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(text, str):</span><br><span class="line">            length = [len(text)]</span><br><span class="line">            result = [self.encode_map[letter] <span class="keyword">for</span> letter <span class="keyword">in</span> text]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            length = []</span><br><span class="line">            result = []</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> text:</span><br><span class="line">                length.append(len(word))</span><br><span class="line">                result.extend([self.encode_map[letter] <span class="keyword">for</span> letter <span class="keyword">in</span> word])</span><br><span class="line">        <span class="keyword">return</span> torch.IntTensor(result), torch.IntTensor(length)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, text_code)</span>:</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">for</span> code <span class="keyword">in</span> text_code:</span><br><span class="line">            word = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(code)):</span><br><span class="line">                <span class="keyword">if</span> code[i] != <span class="number">0</span> <span class="keyword">and</span> (i == <span class="number">0</span> <span class="keyword">or</span> code[i] != code[i<span class="number">-1</span>]):</span><br><span class="line">                    word.append(self.decode_map[code[i]])</span><br><span class="line">            result.append(<span class="string">''</span>.join(word))</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure></p>
<p>这样我们的CRNN的基本流程就搞定了，接下来我们在IIIT-5K上试一试。</p>
<h1 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h1><p>数据集在<a href="http://cvit.iiit.ac.in/projects/SceneTextUnderstanding/IIIT5K.html" target="_blank" rel="noopener">这里</a>下载。</p>
<p>数据集下载下来是.mat文件，还好我大Python有专门的库来加载.mat。<br><em>默默说一句：内存小的小朋友一定要谨慎行事，如果内存只有4G的话（像我一样）就要小心了</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> sio</span><br><span class="line">data = sio.loadmat(<span class="string">'traindata.mat'</span>)</span><br></pre></td></tr></table></figure></p>
<p>可以先观察一波数据集，对训练集来说，有用的数据在<code>data[&#39;traindata&#39;][0]</code>，一共2000条数据，测试集有3000条。其中，每条数据里存的有四项，第一项是图片的文件名，第二项是label（真实标签），第三项第四项分别是大小为50，和1000的字典。数据中的字典十分占内存，他们可以用在转录中过程，本文中并没有使用他们。</p>
<p>pytorch中没有找到现成的API来加载这样的数据，那么我们怎么把数据加载进来呢？比较优雅的做法是继承<code>torch.utils.data.Dataset</code>类，在继承这个类时，必须要重载的方法是<code>__len__</code>和<code>__getitem__</code>。</p>
<ul>
<li><code>__len__</code>使我们的类支持Python内置的<code>len</code>函数</li>
<li><code>__getitem__</code>用来支持取下标运算</li>
</ul>
<p>同时，我们要注意，CRNN要求传入的图片高度相同，宽度至少为100，比较合适的高度是32。所以我们在这里自己定义一个类用来对图片做缩放。类的定义方法参考<code>torchvision.transforms</code>中的类，如下，只需要重载<code>__call__</code>即可。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FixHeightResize</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对图片做固定高度的缩放</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, height=<span class="number">32</span>, minwidth=<span class="number">100</span>)</span>:</span></span><br><span class="line">        self.height = height</span><br><span class="line">        self.minwidth = minwidth</span><br><span class="line"></span><br><span class="line">    <span class="comment"># img 为 PIL.Image 对象</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, img)</span>:</span></span><br><span class="line">        w, h = img.size</span><br><span class="line">        width = max(int(w * self.height / h), self.minwidth)</span><br><span class="line">        <span class="keyword">return</span> img.resize((width, self.height), Image.ANTIALIAS)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IIIT5k</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用于加载IIIT-5K数据集，继承于torch.utils.data.Dataset</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        root (string): 数据集所在的目录</span></span><br><span class="line"><span class="string">        training (bool, optional): 为True时加载训练集，为False时加载测试集，默认为True</span></span><br><span class="line"><span class="string">        fix_width (bool, optional): 为True时将图片缩放到固定宽度，为False时宽度不固定，默认为False</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, training=True, fix_width=False)</span>:</span></span><br><span class="line">        super(IIIT5k, self).__init__()</span><br><span class="line">        data_str = <span class="string">'traindata'</span> <span class="keyword">if</span> training <span class="keyword">else</span> <span class="string">'testdata'</span></span><br><span class="line">        self.img, self.label = zip(*[(x[<span class="number">0</span>][<span class="number">0</span>], x[<span class="number">1</span>][<span class="number">0</span>]) <span class="keyword">for</span> x <span class="keyword">in</span></span><br><span class="line">            sio.loadmat(os.path.join(root, data_str+<span class="string">'.mat'</span>))[data_str][<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 图片缩放 + 转化为灰度图 + 转化为张量</span></span><br><span class="line">        transform = [transforms.Resize((<span class="number">32</span>, <span class="number">100</span>), Image.ANTIALIAS)</span><br><span class="line">                     <span class="keyword">if</span> fix_width <span class="keyword">else</span> FixHeightResize(<span class="number">32</span>)]</span><br><span class="line">        transform.extend([transforms.Grayscale(), transforms.ToTensor()])</span><br><span class="line">        transform = transforms.Compose(transform)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加载图片</span></span><br><span class="line">        self.img = [transform(Image.open(root+<span class="string">'/'</span>+img)) <span class="keyword">for</span> img <span class="keyword">in</span> self.img]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 以下两个方法必须要重载</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self, )</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.img)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.img[idx], self.label[idx]</span><br></pre></td></tr></table></figure></p>
<p>这样就可以像调用<code>torchvision.datasets</code>里的数据集一样方便的调用我们的IIIT-5k了。这里我把图片的加载写在了<code>__init__</code>中，内存消耗较大，大家也可以将图片加载写在<code>__getitem__</code>中，节省内存，不过速度难免会慢一些。在刚才的类里，我们还给IIIT-5K加了一个<code>fix_width</code>参数，至于为什么我们后面会讲。</p>
<p>pytorch提供了一个<code>DataLoader</code>类。将我们之前定义的IIIT5k类的实例传入这个类，可以很方便的加载数据，支持多线程、数据打乱、批训练，何乐而不为呢。</p>
<p>其中，批训练可以明显加快训练过程。不过令人心凉的是，在用DataLoader进行批训练时，pytorch默认会将batch中的张量连接起来，而宽度不固定的图片是不能直接连接的。一个方便的做法是直接将所有图片缩放成统一大小的图片，这就是为什么我们上面加了<code>fix_width</code>这样一个参数。否则我们就只能一张一张的训练了。</p>
<p>为了加快以后加载数据的过程，可以将我们的IIIT-5k实例存入<code>.pkl</code>文件。这样以后加载数据时，省内存，加载更是一秒加载完。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(root, training=True, fix_width=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用于加载IIIT-5K数据集，继承于torch.utils.data.Dataset</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        root (string): 数据集所在的目录</span></span><br><span class="line"><span class="string">        training (bool, optional): 为True时加载训练集，为False时加载测试集，默认为True</span></span><br><span class="line"><span class="string">        fix_width (bool, optional): 为True时将图片缩放到固定宽度，为False时宽度不固定，默认为False</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">        加载的训练集或者测试集</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> training:</span><br><span class="line">        batch_size = <span class="number">128</span> <span class="keyword">if</span> fix_width <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        filename = os.path.join(root, <span class="string">'train'</span>+(<span class="string">'_fix_width'</span> <span class="keyword">if</span> fix_width <span class="keyword">else</span> <span class="string">''</span>)+<span class="string">'.pkl'</span>)</span><br><span class="line">        <span class="keyword">if</span> os.path.exists(filename):</span><br><span class="line">            dataset = pickle.load(open(filename, <span class="string">'rb'</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'==== Loading data.. ===='</span>)</span><br><span class="line">            dataset = IIIT5k(root, training=<span class="keyword">True</span>, fix_width=fix_width)</span><br><span class="line">            pickle.dump(dataset, open(filename, <span class="string">'wb'</span>), <span class="keyword">True</span>)</span><br><span class="line">        dataloader = DataLoader(dataset, batch_size=batch_size, </span><br><span class="line">                                shuffle=<span class="keyword">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        batch_size = <span class="number">128</span> <span class="keyword">if</span> fix_width <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        filename = os.path.join(root, <span class="string">'test'</span>+(<span class="string">'_fix_width'</span> <span class="keyword">if</span> fix_width <span class="keyword">else</span> <span class="string">''</span>)+<span class="string">'.pkl'</span>)</span><br><span class="line">        <span class="keyword">if</span> os.path.exists(filename):</span><br><span class="line">            dataset = pickle.load(open(filename, <span class="string">'rb'</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">'==== Loading data.. ===='</span>)</span><br><span class="line">            dataset = IIIT5k(root, training=<span class="keyword">False</span>, fix_width=fix_width)</span><br><span class="line">            pickle.dump(dataset, open(filename, <span class="string">'wb'</span>), <span class="keyword">True</span>)</span><br><span class="line">        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br></pre></td></tr></table></figure></p>
<h1 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h1><p>有了上面这些，我们就可以开始训练了。优化方法采用Adadelta，对这类自适应优化算法感兴趣的可以看<a href="https://qjy981010.github.io/2017/12/23/%E8%87%AA%E9%80%82%E5%BA%94%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/">我的另一篇博客</a>。（Adadelta算法本身并不需要学习速率，但pytorch给他增加了lr这一参数，这个lr其实就是每次迭代时在参数变化量前乘的系数，默认为1，当作学习速率用即可，但在我这里测试时，lr=1时效果不好，于是改用了0.1）。在固定宽度时，lr设为0.1，速度很快。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(root, start_epoch, epoch_num, letters, net=None, lr=<span class="number">0.1</span>, fix_width=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练CRNN</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        root (str): 存放数据集的文件夹</span></span><br><span class="line"><span class="string">        start_epoch (int): 开始训练的是第多少次epoch，便于对训练过程的追踪回顾。</span></span><br><span class="line"><span class="string">        epoch_num (int): 将训练的epoch数目</span></span><br><span class="line"><span class="string">        letters (str): 所有的字符组成的字符串</span></span><br><span class="line"><span class="string">        net (CRNN, optional): 之前训练过的网络</span></span><br><span class="line"><span class="string">        lr (float, optional): 学习速率，默认为0.1，这里注意adadelta本身没有学习速率</span></span><br><span class="line"><span class="string">                              pytorch增加了这一参数作为每次迭代参数改变量的系数，一般为1，但设为1时测试效果并不好。</span></span><br><span class="line"><span class="string">        fix_width (bool, optional): 是否固定宽度，默认固定</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        CRNN: 训练好的模型</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    trainloader = load_data(root, training=<span class="keyword">True</span>, fix_width=fix_width)</span><br><span class="line">    <span class="comment"># 判断GPU是否可用</span></span><br><span class="line">    use_cuda = torch.cuda.is_available()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> net:</span><br><span class="line">        <span class="comment"># 如果没有之前训练好的模型，就新建一个</span></span><br><span class="line">        net = CRNN(<span class="number">1</span>, len(letters) + <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 损失函数</span></span><br><span class="line">    criterion = CTCLoss()</span><br><span class="line">    <span class="comment"># 优化方法采用Adadelta</span></span><br><span class="line">    optimizer = optim.Adadelta(net.parameters(), lr=lr)</span><br><span class="line">    <span class="keyword">if</span> use_cuda:</span><br><span class="line">        net = net.cuda()</span><br><span class="line">        criterion = criterion.cuda()</span><br><span class="line">    <span class="comment"># 构建编码解码器</span></span><br><span class="line">    labeltransformer = LabelTransformer(letters)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'====   Training..   ===='</span>)</span><br><span class="line">    <span class="comment"># .train() 对批归一化有一定的作用</span></span><br><span class="line">    net.train()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(start_epoch, start_epoch + epoch_num):</span><br><span class="line">        print(<span class="string">'----    epoch: %d    ----'</span> % (epoch, ))</span><br><span class="line">        loss_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, (img, label) <span class="keyword">in</span> enumerate(trainloader):</span><br><span class="line">            label, label_length = labeltransformer.encode(label)</span><br><span class="line">            <span class="keyword">if</span> use_cuda:</span><br><span class="line">                img = img.cuda()</span><br><span class="line">            img, label = Variable(img), Variable(label)</span><br><span class="line">            label_length = Variable(label_length)</span><br><span class="line">            <span class="comment"># 清空梯度</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="comment"># 将图片输入</span></span><br><span class="line">            outputs = net(img)</span><br><span class="line">            output_length = Variable(torch.IntTensor([outputs.size(<span class="number">0</span>)]*outputs.size(<span class="number">1</span>)))</span><br><span class="line">            <span class="comment"># 计算损失</span></span><br><span class="line">            loss = criterion(outputs, label, output_length, label_length)</span><br><span class="line">            <span class="comment"># 反向传播</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            optimizer.step()</span><br><span class="line">            loss_sum += loss.data[<span class="number">0</span>]</span><br><span class="line">        print(<span class="string">'loss = %f'</span> % (loss_sum, ))</span><br><span class="line">    print(<span class="string">'Finished Training'</span>)</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure></p>
<p>为了验证我们模型的效果，还要定义一个测试函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(root, net, letters, fix_width=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    测试CRNN模型</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        root (str): 存放数据集的文件夹</span></span><br><span class="line"><span class="string">        letters (str): 所有的字符组成的字符串</span></span><br><span class="line"><span class="string">        net (CRNN, optional): 训练好的网络</span></span><br><span class="line"><span class="string">        fix_width (bool, optional): 是否固定宽度，默认固定</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    testloader = load_data(root, training=<span class="keyword">False</span>, fix_width=fix_width)</span><br><span class="line">    <span class="comment"># 判断GPU是否可用</span></span><br><span class="line">    use_cuda = torch.cuda.is_available()</span><br><span class="line">    <span class="keyword">if</span> use_cuda:</span><br><span class="line">        net = net.cuda()</span><br><span class="line">    <span class="comment"># 构建编码解码器</span></span><br><span class="line">    labeltransformer = LabelTransformer(letters)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'====    Testing..   ===='</span>)</span><br><span class="line">    <span class="comment"># .eval() 对批归一化有一定的作用</span></span><br><span class="line">    net.eval()</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, (img, origin_label) <span class="keyword">in</span> enumerate(testloader):</span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line">            img = img.cuda()</span><br><span class="line">        img = Variable(img)</span><br><span class="line"></span><br><span class="line">        outputs = net(img) <span class="comment"># length × batch × num_letters</span></span><br><span class="line">        outputs = outputs.max(<span class="number">2</span>)[<span class="number">1</span>].transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># batch × length</span></span><br><span class="line">        outputs = labeltransformer.decode(outputs.data)</span><br><span class="line">        correct += sum([out == real <span class="keyword">for</span> out, real <span class="keyword">in</span> zip(outputs, origin_label)])</span><br><span class="line">    <span class="comment"># 计算准确率</span></span><br><span class="line">    print(<span class="string">'test accuracy: '</span>, correct / <span class="number">30</span>, <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure></p>
<p>还有最后的main函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(training=True, fix_width=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    主函数，控制train与test的调用以及模型的加载存储等</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        training (bool, optional): 为True是训练，为False是测试，默认为True</span></span><br><span class="line"><span class="string">        fix_width (bool, optional): 是否固定图片宽度，默认为False</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    file_name = (<span class="string">'fix_width_'</span> <span class="keyword">if</span> fix_width <span class="keyword">else</span> <span class="string">''</span>) + <span class="string">'crnn.pkl'</span></span><br><span class="line">    letters = <span class="string">'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'</span></span><br><span class="line">    root = <span class="string">'data/IIIT5K/'</span></span><br><span class="line">    <span class="keyword">if</span> training:</span><br><span class="line">        net = <span class="keyword">None</span></span><br><span class="line">        start_epoch = <span class="number">0</span></span><br><span class="line">        epoch_num = <span class="number">2</span> <span class="comment"># 每训练两个epoch进行一次测试</span></span><br><span class="line">        lr = <span class="number">0.1</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(file_name):</span><br><span class="line">            print(<span class="string">'Pre-trained model detected.\nLoading model...'</span>)</span><br><span class="line">            start_epoch, net = pickle.load(open(file_name, <span class="string">'rb'</span>))</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            print(<span class="string">'GPU detected.'</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">            net = train(root, start_epoch, epoch_num, letters, net=net, lr=lr, fix_width=fix_width)</span><br><span class="line">            start_epoch += epoch_num</span><br><span class="line">            test(root, net, letters, fix_width=fix_width)</span><br><span class="line">        <span class="comment"># 将训练的epoch数与我们的模型保存起来，模型还可以加载出来继续训练</span></span><br><span class="line">        pickle.dump((start_epoch, net), open(file_name, <span class="string">'wb'</span>), <span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        start_epoch, net = pickle.load(open(file_name, <span class="string">'rb'</span>))</span><br><span class="line">        test(root, net, letters, fix_width=fix_width)</span><br></pre></td></tr></table></figure></p>
<p>终于，我们可以愉快的训练了：）<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main(training=<span class="keyword">True</span>, fix_width=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p>
<p>全部代码在<a href="https://github.com/qjy981010/CRNN.IIIT-5K.pytorch" target="_blank" rel="noopener">我的github上</a></p>
<p>按论文上的说法，在IIIT-5K数据集上，无字典训练可以达到70%的准确率。我在测试时，如果固定图片高度进行批训练，速度就非常快了，学习速率设为0.1，很快就能把准确率提升到50%左右，但没能达到论文的效果。</p>
<p>如果出现<code>Out of Memory</code>这类错误，请降低加载数据是的<code>batch_size</code>和<code>num_workers</code>。</p>


                
            </div>

            <!-- Comments -->
            
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    


                </div>
            
        </div>
    </div>
</article>

    <!-- Footer -->
    <hr />

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    

                    

                    
                        <li>
                            <a href="https://github.com/qjy981010" target="_blank">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    

                    

                    
                        <li>
                            <a href="mailto:1114943038@qq.com" target="_blank">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-envelope-o fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    

                    
                </ul>
                <p class="copyright text-muted">&copy; 2018 Jiyang Qi<br></p>
                <p class="copyright text-muted">Original Theme <a target="_blank" href="http://startbootstrap.com/template-overviews/clean-blog/">Clean Blog</a> from <a href="http://startbootstrap.com/" target="_blank">Start Bootstrap</a></p>
                <p class="copyright text-muted">Adapted for <a target="_blank" href="https://hexo.io/">Hexo</a> by <a href="http://www.codeblocq.com/" target="_blank">Jonathan Klughertz</a></p>
            </div>
        </div>
    </div>
</footer>


    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->

;


</body>

</html>