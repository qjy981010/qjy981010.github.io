<!DOCTYPE html>
<html lang="zh_CN">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="boom boom boom">
    

    <!--Author-->
    
        <meta name="author" content="Jiyang Qi">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="基本AI术语"/>
    

    <!--Open Graph Description-->
    
        <meta property="og:description" content="boom boom boom" />
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Tensor Lover"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

        <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>基本AI术语 - Tensor Lover</title>

    <!-- Bootstrap Core CSS -->
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"/>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-111519148-1', 'auto');
        ga('send', 'pageview');

    </script>



    <!-- favicon -->
    
    <link rel="icon" href="/img/ai.ico">
    <link rel="shortcut icon" href="/img/ai.ico">
    
	
</head>


<body>

    <!-- Menu -->
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">TensorLover's Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                
                    <li>
                        <a href="/">
                            
                                Home
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/archives">
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/tags">
                            
                                Tags
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/categories">
                            
                                Categories
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="https://github.com/qjy981010">
                            
                                <i class="fa fa-github fa-stack-2x"></i>
                            
                        </a>
                    </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>


    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header" style="background-image: url('http://www.codeblocq.com/assets/projects/hexo-theme-clean-blog/img/home-bg.jpg')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>基本AI术语</h1>
                    
                    <span class="meta">
                        <!-- Date and Author -->
                        
                            Posted by Jiyang Qi on
                        
                        
                            2018-07-09
                        
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Tags and categories -->
           
                <div class="col-lg-4 col-lg-offset-2 col-md-5 col-md-offset-1 post-tags">
                    
                        


<a href="/tags/人工智能/">#人工智能</a> <a href="/tags/AI/">#AI</a> <a href="/tags/术语/">#术语</a> <a href="/tags/基础/">#基础</a>


                    
                </div>
                <div class="col-lg-4 col-md-5 post-categories">
                    
                </div>
            

            <!-- Gallery -->
            

            <!-- Post Main Content -->
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <p>一篇笔记向博客</p>
<h1 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h1><h3 id="极大似然与负对数损失"><a href="#极大似然与负对数损失" class="headerlink" title="极大似然与负对数损失"></a>极大似然与负对数损失</h3><p>极大似然中，一组参数在一堆数据下的似然值，等于每一条数据在这组参数下的条件概率之积。</p>
<p>而我们通过模型得到的置信度，其实就是那个条件概率，所以我们的目标是让他们的积最大。</p>
<p>但损失函数一般是每条数据的损失之和，为了把和变为上面的积，就对每一项取了对数。让对数之和最大，其实就是让负对数之和最小。于是我们便有了负对数损失(NLL Loss)。</p>
<p>$$L(y) = -\log(\hat{y_i})$$</p>
<p><em>注意：pytorch中的NLLLoss本身不进行对数运算，只做一个取负</em></p>
<h3 id="负对数损失与交叉熵损失的关系"><a href="#负对数损失与交叉熵损失的关系" class="headerlink" title="负对数损失与交叉熵损失的关系"></a>负对数损失与交叉熵损失的关系</h3><p>交叉熵损失定义如下，其中$Y$为真实label(0/1)，$\hat{Y}$为预测的结果</p>
<p>$$H(Y,\hat{Y})=-Y\cdot \log (\hat{Y})=-\sum_i y_i \cdot \log (\hat{y_i})$$</p>
<p>将最右边的式子展开，得到的其实就是负对数损失。</p>
<h3 id="softmax与交叉熵"><a href="#softmax与交叉熵" class="headerlink" title="softmax与交叉熵"></a>softmax与交叉熵</h3><p>softmax中的$e^x$与交叉熵中的$\log(x)$抵消，使模型易于收敛。<br>因此softmax通常作为输出层的激活函数</p>
<p><em>注意：pytorch中的CrossEntropyLoss自带softmax</em></p>
<h3 id="为什么要用softmax"><a href="#为什么要用softmax" class="headerlink" title="为什么要用softmax"></a>为什么要用softmax</h3><ol>
<li>当使用交叉熵损失时softmax中的$e^x$与交叉熵中的$\log(x)$抵消，使模型易于收敛。</li>
<li>softmax满足最大熵原则。</li>
</ol>
<hr>
<h1 id="一些术语"><a href="#一些术语" class="headerlink" title="一些术语"></a>一些术语</h1><p>下面主要记录一些基础的术语</p>
<h2 id="信息论-amp-概率论"><a href="#信息论-amp-概率论" class="headerlink" title="信息论&amp;概率论"></a>信息论&amp;概率论</h2><h3 id="熵-Entropy"><a href="#熵-Entropy" class="headerlink" title="熵(Entropy)"></a>熵(Entropy)</h3><p>熵在信息论中可以理解为，一个事件包含的信息量。熵越小，事件包含的信息量就越小。必然事件和不可能事件的熵为0（比如“我是我妈生的”这句话为必然事件，不包含任何信息，信息量为0）。熵定义如下：</p>
<p>$$S(x) = -\sum_i P(x_i) \log_b P(x_i)$$</p>
<p>其中S为熵，x为事件，当x必然发生或必然不发生时，$P(x)$为1或0，此时S为0。</p>
<h3 id="KL散度-KL距离-相对熵-relative-entropy"><a href="#KL散度-KL距离-相对熵-relative-entropy" class="headerlink" title="KL散度/KL距离/相对熵(relative entropy)"></a>KL散度/KL距离/相对熵(relative entropy)</h3><p>KL散度一般用来计算两个概率分布之间的距离，但它与普通的距离计算不同，因为’A对B的KL距离’不一定等于’B对A的KL距离’。</p>
<p>对离散事件： $D_{KL}(A||B) = \sum_i P_A (x_i) \log \frac{P_A (x_i)}{P_B(x_i)} )$</p>
<p>对连续事件： $D_{KL}(A||B) = \int a(x) \log \frac{a(x)}{b(x)}$</p>
<p>如果A与B的概率分布相同，即$P_A=P_B$，则距离为0。</p>
<h3 id="交叉熵-Cross-Entropy"><a href="#交叉熵-Cross-Entropy" class="headerlink" title="交叉熵(Cross Entropy)"></a>交叉熵(Cross Entropy)</h3><p>交叉熵同样用来衡量两个分布之间的差异，其定义如下：</p>
<p>$$H(A,B)= -\sum_i P_A (x_i) \log (P_B (x_i))$$</p>
<p>可以推出来 $D_{KL}(A||B) = -S(A)+H(A,B)$</p>
<h3 id="互信息-mutual-information"><a href="#互信息-mutual-information" class="headerlink" title="互信息(mutual information)"></a>互信息(mutual information)</h3><p>直观上理解，互信息度量X与Y之间共享的信息量。</p>
<p>若 p(x,y) 是X和Y的联合概率分布函数，而p(x)和p(y)分别是X和Y的边缘概率分布函数。则X与Y的互信息定义为：</p>
<p>对离散事件： $I(X;Y) = \sum<em>{x \in X} \sum</em>{y \in Y} p(x,y)\log (\frac{p(x,y)}{p(x)p(y)})$</p>
<p>对连续事件： $I(X;Y) = \int_Y \int_X p(x,y) \log (\frac{p(x,y)}{p(x)p(y)})$</p>
<p>它其实就是计算了联合概率分布与边缘概率分布乘积之间的KL距离。</p>
<p>$$I(X;Y)=D_{KL}(p(x,y)||p(x)p(y))$$</p>
<h3 id="交叉熵与KL散度的关系"><a href="#交叉熵与KL散度的关系" class="headerlink" title="交叉熵与KL散度的关系"></a>交叉熵与KL散度的关系</h3><ul>
<li>相同点：a. 都不具备对称性 b. 都是非负的</li>
<li>当$S(A)$固定时，最小化KL散度与最小化交叉熵等价。而交叉熵公式更简洁，因此我们一般都使用交叉熵。</li>
</ul>
<p>在机器学习中，我们要让模型的分布逼近数据集的分布，而数据集就是上面的A，因此$S(A)$固定</p>
<h2 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h2><h3 id="欧氏距离-Euclidean-distance"><a href="#欧氏距离-Euclidean-distance" class="headerlink" title="欧氏距离(Euclidean distance)"></a>欧氏距离(Euclidean distance)</h3><p>$$d_{12} = \sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$$</p>
<h3 id="曼哈顿距离-Manhattan-Distance"><a href="#曼哈顿距离-Manhattan-Distance" class="headerlink" title="曼哈顿距离(Manhattan Distance)"></a>曼哈顿距离(Manhattan Distance)</h3><p>$$d_{12} = |x_1-x_2|+|y_1-y_2|$$</p>
<h3 id="闵可夫斯基距离-Minkowski-Distance"><a href="#闵可夫斯基距离-Minkowski-Distance" class="headerlink" title="闵可夫斯基距离(Minkowski Distance)"></a>闵可夫斯基距离(Minkowski Distance)</h3><p>可以看做是欧氏距离和曼哈顿距离的一种推广  </p>
<p>$$d_{12} = \sqrt[p]{(x_1-x_2)^p+(y_1-y_2)^p}$$</p>
<p>当$p=1$时，就是曼哈顿距离<br>当$p=2$时，就是欧氏距离<br>当$p \to \infty$时，就是切比雪夫距离  </p>
<h3 id="马氏距离-Mahalanobis-Distance"><a href="#马氏距离-Mahalanobis-Distance" class="headerlink" title="马氏距离(Mahalanobis Distance)"></a>马氏距离(Mahalanobis Distance)</h3><p>前面的距离度量均会受到变量量纲的影响，而马氏距离通过对数据的仿射变换，消除了这种影响。这是它的优点，而在某些情况下也会成为它的缺点。</p>
<p>定义：有m个样本向量$X_1, X_m$，协方差矩阵记为S，则其中向量$X_i$与$X_j$之间的马氏距离定义为：</p>
<p>$$D(X_i,X_j) = \sqrt{(X_i-X_j)^TS^{-1}(X_i-X_j)}$$</p>
<h3 id="余弦距离-Cosine-Distance"><a href="#余弦距离-Cosine-Distance" class="headerlink" title="余弦距离(Cosine Distance)"></a>余弦距离(Cosine Distance)</h3><p>与欧氏距离相比，余弦距离对各个特征之间的相对大小更为敏感，对绝对大小却不怎么敏感。在人脸检索等领域，貌似余弦距离与欧氏距离表现差不多？不过毕竟欧氏距离更直观，所以除一些特定问题外还是用欧氏距离吧。</p>
<p>对它比较有兴趣的可以看看<a href="https://cmry.github.io/notes/euclidean-v-cosine" target="_blank" rel="noopener">这个</a></p>
<h3 id="编辑距离-Levenshtein-Distance"><a href="#编辑距离-Levenshtein-Distance" class="headerlink" title="编辑距离(Levenshtein Distance)"></a>编辑距离(Levenshtein Distance)</h3><p>用来定义两个字符串之间的距离，指两个字串之间，由一个转成另一个所需的最少编辑操作次数。Standardization字符，删除字符。</p>
<h3 id="杰卡德距离-Jaccard-Distance"><a href="#杰卡德距离-Jaccard-Distance" class="headerlink" title="杰卡德距离(Jaccard Distance)"></a>杰卡德距离(Jaccard Distance)</h3><p>用于衡量集合间的距离，定义为两集合的交集与并集中元素数量的比值。物体检测中的IoU其实就是杰拉德距离。</p>
<p>$$J(A,B) = \frac{|A\bigcap B|}{|A\bigcup B|}$$</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><h3 id="归一化-Normalization"><a href="#归一化-Normalization" class="headerlink" title="归一化(Normalization)"></a>归一化(Normalization)</h3><p>通过对原始数据进行线性变换把数据映射到[0,1]之间</p>
<p>$$x’ = \frac{x-min}{max-min}$$</p>
<h3 id="标准化-Standardization"><a href="#标准化-Standardization" class="headerlink" title="标准化(Standardization)"></a>标准化(Standardization)</h3><p>使数据均值为0，标准差为1</p>
<p>$$x’ = \frac{x-\mu}{\sigma}$$</p>
<h2 id="评价标准"><a href="#评价标准" class="headerlink" title="评价标准"></a>评价标准</h2><p>首先定义基本的评价标准</p>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">预测为真</th>
<th style="text-align:center">预测为假</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">实际为真</td>
<td style="text-align:center">TP(真正例)</td>
<td style="text-align:center">FN(假反例)</td>
</tr>
<tr>
<td style="text-align:center">实际为假</td>
<td style="text-align:center">FP(假正例)</td>
<td style="text-align:center">TN(真反例)</td>
</tr>
</tbody>
</table>
<p>接下来的评价标准基本都基于上面这四个</p>
<h3 id="精确率-查准率-precision"><a href="#精确率-查准率-precision" class="headerlink" title="精确率/查准率(precision)"></a>精确率/查准率(precision)</h3><p>$$P = \frac{TP}{TP+FP}$$</p>
<h3 id="召回率-查全率-recall"><a href="#召回率-查全率-recall" class="headerlink" title="召回率/查全率(recall)"></a>召回率/查全率(recall)</h3><p>$$R = \frac{TP}{TP+FN}$$</p>
<h3 id="准确率-accuracy"><a href="#准确率-accuracy" class="headerlink" title="准确率(accuracy)"></a>准确率(accuracy)</h3><p>$$Acc = \frac{TP+TN}{TP+FP+TN+FN}$$</p>
<h3 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1 Score"></a>F1 Score</h3><p>通常我们需要综合精确率和召回率进行考虑，这时我们一般使用F1度量  </p>
<p>$$F_1 = \frac{2 \times P \times R}{P + R} =  \frac{2 \times TP}{2 \times TP + FP + FN}$$</p>
<p>当对精确率和召回率的重视程度不同时，我们可以采用更一般的$F_\beta$</p>
<p>$$F_\beta = \frac{(1+\beta^2) \times P \times R}{(\beta^2 \times P) + R}$$</p>
<h3 id="P-R曲线"><a href="#P-R曲线" class="headerlink" title="P-R曲线"></a>P-R曲线</h3><p>以召回率为横轴，精确率为纵轴，得到的曲线为P-R曲线。</p>
<div align="center"><br><img src="/images/P-R" alt=""><br></div>

<p>当对两个模型进行比较时，我们可以根据他们P-R曲线下的面积(AP)判断，面积越大性能越好。</p>
<h3 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h3><p>先定义真正例率和假正例率：</p>
<p>$$TPR = \frac{TP}{TP+FN}$$</p>
<p>$$FPR = \frac{FP}{TN+FP}$$</p>
<p>以FPR为横轴，TPR为纵轴即可得到ROC曲线。</p>
<div align="center"><br><img src="/images/roc.png" alt=""><br></div>

<h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><p>ROC曲线下的面积即为AUC，越大说明模型越好。<strong>可以无视正负样本不平衡的影响</strong></p>
<h3 id="mAP-mean-average-precision"><a href="#mAP-mean-average-precision" class="headerlink" title="mAP(mean average precision)"></a>mAP(mean average precision)</h3><p>常用于多类别物体检测。对每个类别均可以绘制一条P-R曲线，其中AP即指P-R曲线下的面积。mAP即为多个类别AP的平均值。</p>
<h2 id="奇怪的卷积"><a href="#奇怪的卷积" class="headerlink" title="奇怪的卷积"></a>奇怪的卷积</h2><h3 id="反卷积-转置卷积-deconvolution-transposed-convolution"><a href="#反卷积-转置卷积-deconvolution-transposed-convolution" class="headerlink" title="反卷积/转置卷积 (deconvolution / transposed convolution)"></a>反卷积/转置卷积 (deconvolution / transposed convolution)</h3><p>可以理解为一种特殊的卷积，只不过它会将feature map放大，达到上采样的效果。  </p>
<div align="center"><br><img src="/images/no_padding_no_strides_transposed.gif" alt=""><br></div>

<p>上图可以理解3x3的卷积核，padding=2，stride=1的卷积操作。而同时，在特征图大小变化上，与padding=0，stride=1的卷积操作相反。<br>用pytorch实现时代码如下，其中kernel_size为3，stride与padding按其对应的卷积操作参数进行设置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>当卷积操作stride大于1时，此时其对应的转置卷积的stride小于1，此时被称为分数卷积(fractionally-strided convolution)，如下图：</p>
<div align="center"><br><img src="/images/no_padding_strides_transposed.gif" alt=""><br></div>

<p><a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank" rel="noopener">for more .gif</a></p>
<h3 id="空洞卷积-扩张卷积-Dilated-Convolutions"><a href="#空洞卷积-扩张卷积-Dilated-Convolutions" class="headerlink" title="空洞卷积/扩张卷积(Dilated Convolutions)"></a>空洞卷积/扩张卷积(Dilated Convolutions)</h3><p>空洞卷积可以在基本不增加计算量的前提下，增大感受野。之前如果需要扩大感受野，都是采用池化，但池化会造成分辨率下降。</p>
<div align="center"><br><img src="/images/dilation.gif" alt=""><br></div>

<p>为了不损失数据的连续性，我们需要选择适当的各层空洞卷积参数，如下图的配置方案。</p>
<div align="center"><br><img src="/images/dilated.png" alt=""><br></div>

<p>pytorch中实现有一个空洞的卷积操作，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>其中dilation参数默认为1，为1在pytorch中定义为没有空洞。不同框架中对此参数的定义不同，需要注意。</p>
<p>空洞卷积感受野大的特点使其可以处理长序列问题，与RNN平起平坐。</p>
<h2 id="深度学习的各种归一化"><a href="#深度学习的各种归一化" class="headerlink" title="深度学习的各种归一化"></a>深度学习的各种归一化</h2><h3 id="BN-LN-IN-GN-SN"><a href="#BN-LN-IN-GN-SN" class="headerlink" title="BN/LN/IN/GN/SN"></a>BN/LN/IN/GN/SN</h3><div align="center"><br><img src="/images/bn_ln_in_gn.png" alt=""><br></div>

<ul>
<li>batchNorm是在batch上，对NHW做归一化，对小batchsize效果不好；</li>
<li>layerNorm在通道方向上，对CHW归一化，主要对RNN作用明显；</li>
<li>instanceNorm在图像像素上，对HW做归一化，用在风格化迁移；</li>
<li>GroupNorm将channel分组，然后再做归一化；</li>
<li>SwitchableNorm是将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法。</li>
</ul>
<h2 id="机器学习问题"><a href="#机器学习问题" class="headerlink" title="机器学习问题"></a>机器学习问题</h2><h3 id="维数灾难"><a href="#维数灾难" class="headerlink" title="维数灾难"></a>维数灾难</h3><p>维数（特征）增多主要会带来的高维空间数据稀疏化问题，即有限的数据在高维空间下更稀疏。当数据维度增高，各个维度之间可能的组合数呈指数增长，我们所拥有的数据覆盖了一部分组合，但其覆盖的百分比越来越小。所以当数据维数（特征）较多而数据较少时易造成过拟合。</p>
<p>同时，高维空间下，距离计算更加困难，而且最大与最小欧氏距离之间的相对差距会越来越小($\lim_{d \to \infty} \frac{D_{max}-D_{min}}{D_{min}} \to 0$)，所以收敛会变慢。</p>
<hr>
<h1 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h1><h3 id="预处理一般步骤"><a href="#预处理一般步骤" class="headerlink" title="预处理一般步骤"></a>预处理一般步骤</h3><p>一般需要对数据进行：</p>
<ol>
<li>去除无用属性，如每个人的id</li>
<li>填补缺失值</li>
<li>特征编码，对非数值特征进行独热编码（若此特征只有两种取值，可以直接0/1）</li>
<li>数据标准化</li>
<li>打乱数据</li>
<li>可能需要降维</li>
<li>划分训练集、验证集、测试集</li>
</ol>
<h2 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h2><h3 id="batchsize大小"><a href="#batchsize大小" class="headerlink" title="batchsize大小"></a>batchsize大小</h3><p>一般来讲：</p>
<ul>
<li>batchsize越大，训练越稳定</li>
<li>在一定范围内，增大batchsize不会增加每个batch的运算时间</li>
<li>小batchsize相比大batchsize，相当于在每个iter中增加了噪声，起到一定的正则化效果，可以减小泛化误差。但同时为了维持稳定，需要更小的学习速率，训练更慢</li>
</ul>
<h3 id="提速"><a href="#提速" class="headerlink" title="提速"></a>提速</h3><ul>
<li>GPU占用率&lt;&lt;100%通常表明，此时系统在对数据进行读写或预处理。若这部分时间占用较大，可以考虑提前进行不带随机数的预处理（如转化为灰度图、resize等），并将结果以.pkl文件保存。可以减少IO及预处理时间</li>
<li>在内存足够的情况下多进程读取数据</li>
<li>可以尝试去寻找一个合适的<strong>绝对最小批量</strong>，batchsize低于它时，不会减少每个batch的运算时间。- 同时batchsize为2的幂时可能会较快</li>
</ul>
<p><em>可能存在的误区：训练快慢并不能只看每个epoch需要多少时间，当batchsize增大，每个epoch所需时间减少，但每个epoch内的iter次数（参数更新次数）也减少，不一定收敛快。一般在保证每个iter所需时间几乎最短的情况下增大batchsize即可。</em></p>
<h3 id="模型保存"><a href="#模型保存" class="headerlink" title="模型保存"></a>模型保存</h3><ul>
<li>日常训练时暂存模型，尽量使用框架提供的方法，仅保存参数。当得到一个较好的模型时，可以考虑将其以.pkl形式保存一份（若定义网络模型的代码有改动，只保存参数会导致无法方便的把参数加载进去）</li>
<li>保存模型参数时一般同时保存当前的epoch数、优化器的参数</li>
</ul>
<h3 id="延拓法"><a href="#延拓法" class="headerlink" title="延拓法"></a>延拓法</h3><p>使用多个损失函数，可以让模型不那么容易被限制在局部最小值点。</p>


                
            </div>

            <!-- Comments -->
            
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    


                </div>
            
        </div>
    </div>
</article>

    <!-- Footer -->
    <hr />

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    

                    

                    
                        <li>
                            <a href="https://github.com/qjy981010" target="_blank">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    

                    

                    
                        <li>
                            <a href="mailto:1114943038@qq.com" target="_blank">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-envelope-o fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    

                    
                </ul>
                <p class="copyright text-muted">&copy; 2018 Jiyang Qi<br></p>
                <p class="copyright text-muted">Original Theme <a target="_blank" href="http://startbootstrap.com/template-overviews/clean-blog/">Clean Blog</a> from <a href="http://startbootstrap.com/" target="_blank">Start Bootstrap</a></p>
                <p class="copyright text-muted">Adapted for <a target="_blank" href="https://hexo.io/">Hexo</a> by <a href="http://www.codeblocq.com/" target="_blank">Jonathan Klughertz</a></p>
            </div>
        </div>
    </div>
</footer>


    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->

;
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
        tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
        TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
        messageStyle: "none"
    }); 
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
;
;


</body>

</html>